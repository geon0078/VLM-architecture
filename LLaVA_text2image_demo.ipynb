{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4227bff3",
   "metadata": {},
   "source": [
    "# LLaVA 이미지 분석 및 질문-답변 실습 (공식 저장소 기반)\n",
    "\n",
    "이 노트북은 [LLaVA 공식 저장소](https://github.com/haotian-liu/LLaVA)를 사용하여 이미지를 분석하고 질문에 답하는 과정을 단계별로 실습합니다.\n",
    "\n",
    "**중요:** LLaVA는 텍스트→이미지 생성 모델이 아닙니다. LLaVA는 이미지를 분석하고 질문에 대답하는 비전-언어 모델입니다.\n",
    "\n",
    "## 목차\n",
    "1. LLaVA 라이브러리 설치 및 임포트\n",
    "2. 모델 로드\n",
    "3. 이미지와 질문 준비\n",
    "4. LLaVA 모델 실행 (이미지 분석)\n",
    "5. 결과 시각화\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 필요 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8870e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAMSUNG\\anaconda3\\envs\\llava\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# LLaVA 및 의존 라이브러리 임포트\n",
    "import torch\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733d311",
   "metadata": {},
   "source": [
    "## 2. 모델 및 프로세서 로드\n",
    "\n",
    "LLaVA 공식 API를 사용하여 사전학습된 모델과 프로세서를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c0b7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "import torch\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    device_map=\"auto\",  # 혹은 수동 설정\n",
    "    offload_folder=\"./offload\"  # 디스크에 weight 저장할 경로\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881e4c7",
   "metadata": {},
   "source": [
    "## 3. 이미지와 질문 준비\n",
    "\n",
    "LLaVA에 입력할 이미지와 질문을 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7126289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "준비된 질문: 이 이미지에 있는 동물에 대해 자세히 설명해주세요.\n",
      "분석할 이미지: img/cat-1192026_1280.jpg\n"
     ]
    }
   ],
   "source": [
    "# 이미지에 대한 질문 준비\n",
    "query = \"이 이미지에 있는 동물에 대해 자세히 설명해주세요.\"\n",
    "print(f\"준비된 질문: {query}\")\n",
    "\n",
    "# 사용할 이미지 경로 설정\n",
    "image_path = \"img/cute_kitten.jpg\"  # 업로드된 귀여운 새끼고양이 이미지\n",
    "print(f\"분석할 이미지: {image_path}\")\n",
    "\n",
    "# 텍스트 프롬프트 입력\n",
    "prompt = input(\"cute cat on the beach, sunny day, high quality, 4k resolution, realistic style, vibrant colors, detailed fur, playful expression\")\n",
    "\n",
    "# 토크나이저를 사용한 전처리\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# 이미지에 대한 다양한 질문들 준비\n",
    "questions = [\n",
    "    \"이 이미지에 있는 동물에 대해 자세히 설명해주세요.\",\n",
    "    \"이 고양이의 나이는 대략 몇 살로 보이나요?\",\n",
    "    \"이 고양이의 털 색깔과 무늬를 설명해주세요.\",\n",
    "    \"이 고양이의 표정이나 감정 상태는 어떠한가요?\",\n",
    "    \"이 사진은 어떤 환경에서 촬영된 것 같나요?\"\n",
    "]\n",
    "\n",
    "# 기본 질문 선택\n",
    "selected_question = questions[0]\n",
    "print(f\"선택된 질문: {selected_question}\")\n",
    "print(f\"분석할 이미지: {image_path}\")\n",
    "\n",
    "# 이미지 파일 존재 확인\n",
    "import os\n",
    "if os.path.exists(image_path):\n",
    "    print(\"✓ 이미지 파일이 존재합니다.\")\n",
    "else:\n",
    "    print(\"✗ 이미지 파일을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fb291",
   "metadata": {},
   "source": [
    "## 4. LLaVA 모델 실행 (이미지 분석)\n",
    "\n",
    "**중요:** LLaVA는 텍스트→이미지 생성 모델이 아닙니다. LLaVA는 이미지를 분석하고 질문에 대답하는 비전-언어 모델입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96141b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval_model() got an unexpected keyword argument 'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이 이미지에 있는 동물에 대해 자세히 설명해주세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# eval_model 함수 사용 (이미지 분석 및 텍스트 생성)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLaVA 응답:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n",
      "\u001b[1;31mTypeError\u001b[0m: eval_model() got an unexpected keyword argument 'model_path'"
     ]
    }
   ],
   "source": [
    "# LLaVA 모델을 사용한 이미지 분석 및 질문-답변 (예시)\n",
    "# LLaVA는 텍스트-이미지 변환 기능이 내장되어 있지 않으며,\n",
    "# 주로 이미지를 분석하고 이에 대한 질문에 답하는 비전-언어 모델입니다.\n",
    "# 텍스트-이미지 생성을 원할 경우 Stable Diffusion, DALL-E 등의 모델을 사용해야 합니다.\n",
    "\n",
    "# 이미지 분석 및 질문-답변 실행\n",
    "# (결과는 result 변수에 저장됨)\n",
    "# 예시: eval_model 함수 사용\n",
    "# 참고: eval_model 함수의 인자와 반환값은 LLaVA 버전에 따라 다를 수 있습니다.\n",
    "\n",
    "# 예시 입력 이미지\n",
    "image_path = \"img/cat-1192026_1280.jpg\"  # 분석할 이미지 경로\n",
    "\n",
    "# 이미지에 대한 질문\n",
    "query = \"이 이미지에 있는 동물에 대해 자세히 설명해주세요.\"\n",
    "\n",
    "# LLaVA 모델을 사용한 이미지 분석 및 질문-답변\n",
    "print(f\"이미지 분석 중: {image_path}\")\n",
    "print(f\"질문: {selected_question}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # eval_model 함수 사용하여 이미지 분석\n",
    "    result = eval_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=model_name,\n",
    "        query=selected_question,\n",
    "        image_file=image_path,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        image_processor=image_processor,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    \n",
    "    print(\"LLaVA 분석 결과:\")\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n",
    "    print(\"모델 로딩이나 이미지 분석 중 문제가 발생했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a370c",
   "metadata": {},
   "source": [
    "## 5. 결과 시각화\n",
    "\n",
    "입력 이미지와 LLaVA의 텍스트 응답을 함께 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35176a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 이미지 시각화 및 저장\n",
    "if generated_image is not None:\n",
    "    if isinstance(generated_image, np.ndarray):\n",
    "        img = Image.fromarray((generated_image * 255).astype(np.uint8))\n",
    "    elif isinstance(generated_image, Image.Image):\n",
    "        img = generated_image\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 이미지 타입입니다.\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('LLaVA 생성 이미지')\n",
    "    plt.show()\n",
    "    img.save('generated_image_llava.png')\n",
    "    print('이미지가 generated_image_llava.png 파일로 저장되었습니다.')\n",
    "else:\n",
    "    print('생성된 이미지가 없습니다.')\n",
    "\n",
    "# 입력 이미지 및 LLaVA 응답 시각화\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    # 입력 이미지 표시\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 이미지 표시\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('분석 대상 이미지: 귀여운 새끼고양이', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # LLaVA 응답 표시\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.text(0.05, 0.95, f\"질문: {selected_question}\", \n",
    "             fontsize=12, fontweight='bold', ha='left', va='top', \n",
    "             transform=plt.gca().transAxes, wrap=True)\n",
    "    \n",
    "    plt.text(0.05, 0.8, f\"LLaVA 응답:\\n{result}\", \n",
    "             fontsize=11, ha='left', va='top', \n",
    "             transform=plt.gca().transAxes, wrap=True)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title('LLaVA 분석 결과', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 결과 요약 출력\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🐱 LLaVA 이미지 분석 완료\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📷 이미지: {image_path}\")\n",
    "    print(f\"❓ 질문: {selected_question}\")\n",
    "    print(f\"💬 응답: {result}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ 이미지 파일을 찾을 수 없습니다: {image_path}\")\n",
    "    print(\"이미지를 img/ 폴더에 저장했는지 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 분석: 여러 질문으로 이미지 분석하기\n",
    "print(\"🔍 여러 질문으로 이미지를 더 자세히 분석해보겠습니다...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, question in enumerate(questions[1:], 1):  # 첫 번째 질문은 이미 분석했으므로 제외\n",
    "    print(f\"\\n질문 {i+1}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        result = eval_model(\n",
    "            model_path=model_path,\n",
    "            model_base=None,\n",
    "            model_name=model_name,\n",
    "            query=question,\n",
    "            image_file=image_path,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            image_processor=image_processor,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        print(f\"답변: {result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n🎉 모든 질문에 대한 분석이 완료되었습니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
